{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMghJ51wUiISUowsdhUIkek"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"6Brc5Yb_T442","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741845531428,"user_tz":-540,"elapsed":10,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}},"outputId":"377d2d6c-678b-4d94-9645-196d3cb429a9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["9223372036854775807"]},"metadata":{},"execution_count":1}],"source":["import sys\n","\n","sys.maxsize"]},{"cell_type":"markdown","source":["파이썬은 기본적으로 데이터를 한 개씩 올리는 방법을 지원\n","\n","- iterator/iterable\n","- generator\n","\n","데이터를 한꺼번에 메모리에 올림, 큰 데이터에는 부적합함.\n","- comprehension"],"metadata":{"id":"dmVD5TafGGrf"}},{"cell_type":"markdown","source":["### Lazy"],"metadata":{"id":"QY7laqbNIJIC"}},{"cell_type":"code","source":["a = [1,2,3,4,5]\n","b = iter(a)\n","b"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Ovm3ONAFWgk","executionInfo":{"status":"ok","timestamp":1741846213048,"user_tz":-540,"elapsed":5,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}},"outputId":"6cefeefb-cd86-4355-94be-677287a60d6c"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<list_iterator at 0x7b6624bf3700>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# b[0] # 아직 메모리에 접근하지 않았기 때문에 접근 불가. next() 등을 써야 함.\n","\n","# iter 하기 싫으면 for에 break를 걸어 하나만 확인한다."],"metadata":{"id":"PVmfZZ8LGuU6","executionInfo":{"status":"ok","timestamp":1741846213961,"user_tz":-540,"elapsed":3,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["next(b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oY_7vljuG4M2","executionInfo":{"status":"ok","timestamp":1741846072823,"user_tz":-540,"elapsed":10,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}},"outputId":"1f6467ff-6543-4739-81cd-1590f9b20581"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["next(b) # 메모리에 올려놓고 휘발함, 실행시점에 따라 값이 달라진다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QSIhV_i2Hc2D","executionInfo":{"status":"ok","timestamp":1741846078323,"user_tz":-540,"elapsed":7,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}},"outputId":"c89338d3-816e-472a-c386-44735d60db11"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["for i in b:\n","  print(i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KhvvemzdHeLo","executionInfo":{"status":"ok","timestamp":1741846157359,"user_tz":-540,"elapsed":7,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}},"outputId":"ca2da288-5f6f-4dcf-b156-1cad7274ff89"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n","4\n","5\n"]}]},{"cell_type":"code","source":["for i in b: # b의 구조체가 모두 날아감.\n","  print(i)"],"metadata":{"id":"gHDybcNZHxfH","executionInfo":{"status":"ok","timestamp":1741846173288,"user_tz":-540,"elapsed":41,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["a = [1,2,3,4,5]\n","b = iter(a)\n","next(b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1AEBQ17UH1Xj","executionInfo":{"status":"ok","timestamp":1741846231000,"user_tz":-540,"elapsed":3,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}},"outputId":"0fd9adb2-9181-4c34-84ae-c4852f00c634"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["list(b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bp9Yn8mSIDdo","executionInfo":{"status":"ok","timestamp":1741846246493,"user_tz":-540,"elapsed":4,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}},"outputId":"756115e1-1b69-4336-e3f5-8ef993515192"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 3, 4, 5]"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# Generic Function: 타입에 상관없이 입력을 받을 수 있는 함수\n","from functools import singledispatch\n","\n","# 다형성: 이름은 같지만 다르게 동작함.\n","@singledispatch\n","def x(t):\n","  return t\n","\n","@x.register(int) # x에서 int가 들어오면 이것을 실행\n","def _(t):\n","  print('int')\n","  return t\n","\n","@x.register(str) # x에서 str이 들어오면 이것을 실행\n","def _(t):\n","  print('str')\n","  return t"],"metadata":{"id":"C1g3Ra2NIHQB","executionInfo":{"status":"ok","timestamp":1741846731411,"user_tz":-540,"elapsed":1,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["x('s')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"C0o272p_JZYB","executionInfo":{"status":"ok","timestamp":1741846731738,"user_tz":-540,"elapsed":5,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}},"outputId":"07de2602-64d6-4196-a49b-c3ebf4c8ebe8"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["str\n"]},{"output_type":"execute_result","data":{"text/plain":["'s'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["x(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1aCdXxrnJqC2","executionInfo":{"status":"ok","timestamp":1741846732082,"user_tz":-540,"elapsed":40,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}},"outputId":"1afaaa8c-c72a-41a6-acf9-28fb3efcb120"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["int\n"]},{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["# Sequence"],"metadata":{"id":"JcQXpXz2RSp6"}},{"cell_type":"code","source":["import tensorflow as tf\n","import inspect\n","\n","print(inspect.getsource(tf.keras.utils.Sequence))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YjISS8ePJ0Mg","executionInfo":{"status":"ok","timestamp":1741848720660,"user_tz":-540,"elapsed":7082,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}},"outputId":"8deba2bd-422b-44ec-c46b-c076a97ddb29"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["@keras_export([\"keras.utils.PyDataset\", \"keras.utils.Sequence\"])\n","class PyDataset:\n","    \"\"\"Base class for defining a parallel dataset using Python code.\n","\n","    Every `PyDataset` must implement the `__getitem__()` and the `__len__()`\n","    methods. If you want to modify your dataset between epochs,\n","    you may additionally implement `on_epoch_end()`,\n","    or `on_epoch_begin` to be called at the start of each epoch.\n","    The `__getitem__()` method should return a complete batch\n","    (not a single sample), and the `__len__` method should return\n","    the number of batches in the dataset (rather than the number of samples).\n","\n","    Args:\n","        workers: Number of workers to use in multithreading or\n","            multiprocessing.\n","        use_multiprocessing: Whether to use Python multiprocessing for\n","            parallelism. Setting this to `True` means that your\n","            dataset will be replicated in multiple forked processes.\n","            This is necessary to gain compute-level (rather than I/O level)\n","            benefits from parallelism. However it can only be set to\n","            `True` if your dataset can be safely pickled.\n","        max_queue_size: Maximum number of batches to keep in the queue\n","            when iterating over the dataset in a multithreaded or\n","            multiprocessed setting.\n","            Reduce this value to reduce the CPU memory consumption of\n","            your dataset. Defaults to 10.\n","\n","    Notes:\n","\n","    - `PyDataset` is a safer way to do multiprocessing.\n","        This structure guarantees that the model will only train\n","        once on each sample per epoch, which is not the case\n","        with Python generators.\n","    - The arguments `workers`, `use_multiprocessing`, and `max_queue_size`\n","        exist to configure how `fit()` uses parallelism to iterate\n","        over the dataset. They are not being used by the `PyDataset` class\n","        directly. When you are manually iterating over a `PyDataset`,\n","        no parallelism is applied.\n","\n","    Example:\n","\n","    ```python\n","    from skimage.io import imread\n","    from skimage.transform import resize\n","    import numpy as np\n","    import math\n","\n","    # Here, `x_set` is list of path to the images\n","    # and `y_set` are the associated classes.\n","\n","    class CIFAR10PyDataset(keras.utils.PyDataset):\n","\n","        def __init__(self, x_set, y_set, batch_size, **kwargs):\n","            super().__init__(**kwargs)\n","            self.x, self.y = x_set, y_set\n","            self.batch_size = batch_size\n","\n","        def __len__(self):\n","            # Return number of batches.\n","            return math.ceil(len(self.x) / self.batch_size)\n","\n","        def __getitem__(self, idx):\n","            # Return x, y for batch idx.\n","            low = idx * self.batch_size\n","            # Cap upper bound at array length; the last batch may be smaller\n","            # if the total number of items is not a multiple of batch size.\n","            high = min(low + self.batch_size, len(self.x))\n","            batch_x = self.x[low:high]\n","            batch_y = self.y[low:high]\n","\n","            return np.array([\n","                resize(imread(file_name), (200, 200))\n","                   for file_name in batch_x]), np.array(batch_y)\n","    ```\n","    \"\"\"\n","\n","    def __init__(self, workers=1, use_multiprocessing=False, max_queue_size=10):\n","        self._workers = workers\n","        self._use_multiprocessing = use_multiprocessing\n","        self._max_queue_size = max_queue_size\n","\n","    def _warn_if_super_not_called(self):\n","        warn = False\n","        if not hasattr(self, \"_workers\"):\n","            self._workers = 1\n","            warn = True\n","        if not hasattr(self, \"_use_multiprocessing\"):\n","            self._use_multiprocessing = False\n","            warn = True\n","        if not hasattr(self, \"_max_queue_size\"):\n","            self._max_queue_size = 10\n","            warn = True\n","        if warn:\n","            warnings.warn(\n","                \"Your `PyDataset` class should call \"\n","                \"`super().__init__(**kwargs)` in its constructor. \"\n","                \"`**kwargs` can include `workers`, \"\n","                \"`use_multiprocessing`, `max_queue_size`. Do not pass \"\n","                \"these arguments to `fit()`, as they will be ignored.\",\n","                stacklevel=2,\n","            )\n","\n","    @property\n","    def workers(self):\n","        self._warn_if_super_not_called()\n","        return self._workers\n","\n","    @workers.setter\n","    def workers(self, value):\n","        self._workers = value\n","\n","    @property\n","    def use_multiprocessing(self):\n","        self._warn_if_super_not_called()\n","        return self._use_multiprocessing\n","\n","    @use_multiprocessing.setter\n","    def use_multiprocessing(self, value):\n","        self._use_multiprocessing = value\n","\n","    @property\n","    def max_queue_size(self):\n","        self._warn_if_super_not_called()\n","        return self._max_queue_size\n","\n","    @max_queue_size.setter\n","    def max_queue_size(self, value):\n","        self._max_queue_size = value\n","\n","    def __getitem__(self, index):\n","        \"\"\"Gets batch at position `index`.\n","\n","        Args:\n","            index: position of the batch in the PyDataset.\n","\n","        Returns:\n","            A batch\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    @property\n","    def num_batches(self):\n","        \"\"\"Number of batches in the PyDataset.\n","\n","        Returns:\n","            The number of batches in the PyDataset or `None` to indicate that\n","            the dataset is infinite.\n","        \"\"\"\n","        # For backwards compatibility, support `__len__`.\n","        if hasattr(self, \"__len__\"):\n","            return len(self)\n","        raise NotImplementedError(\n","            \"You need to implement the `num_batches` property:\\n\\n\"\n","            \"@property\\ndef num_batches(self):\\n  return ...\"\n","        )\n","\n","    def on_epoch_begin(self):\n","        \"\"\"Method called at the beginning of every epoch.\"\"\"\n","        pass\n","\n","    def on_epoch_end(self):\n","        \"\"\"Method called at the end of every epoch.\"\"\"\n","        pass\n","\n"]}]},{"cell_type":"code","source":["# 이 두 개를 구현하면 sequencial 하게 사용할 수 있다.\n","# __getitem__ / __len__ : protocol <- duck typing"],"metadata":{"id":"f2qDFwVlRhjc","executionInfo":{"status":"ok","timestamp":1741848934994,"user_tz":-540,"elapsed":6,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["### Sequenctial data를 만들 수 있는 방법 3가지\n","\n","1. duck typing\n","- 그냥 그런 듯이 사용하면 됨\n","\n","2. 상속\n","- 상속을 받으면 추가기능을 **처음부터 만들 필요가 없고, 재사용이 가능하다**.\n","- duck typing보다 빠르게, 추가 기능만 구현할 수 있다.\n","- 부모가 바뀌면 기능들을 모두 바꿔야 하는 문제가 있다.\n","- 덩치가 커져서 메모리를 많이 사용할 수 있다.\n","\n","3."],"metadata":{"id":"FgYb7vauSeG6"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","dir(Dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3cLh7obSXnZ","executionInfo":{"status":"ok","timestamp":1741849243574,"user_tz":-540,"elapsed":7,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}},"outputId":"8a7c04cc-9966-4b1e-a4be-c93d9c340f8c"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['__add__',\n"," '__annotations__',\n"," '__class__',\n"," '__class_getitem__',\n"," '__delattr__',\n"," '__dict__',\n"," '__dir__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__getitem__',\n"," '__getstate__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__module__',\n"," '__ne__',\n"," '__new__',\n"," '__orig_bases__',\n"," '__parameters__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__slots__',\n"," '__str__',\n"," '__subclasshook__',\n"," '__weakref__',\n"," '_is_protocol']"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["class D(Dataset):\n","  pass\n","\n","print(inspect.getsource(Dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F1gO0XqsTi9H","executionInfo":{"status":"ok","timestamp":1741849339457,"user_tz":-540,"elapsed":12,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}},"outputId":"c5a150c7-3d57-49d1-bb63-559070ad32a5"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["class Dataset(Generic[_T_co]):\n","    r\"\"\"An abstract class representing a :class:`Dataset`.\n","\n","    All datasets that represent a map from keys to data samples should subclass\n","    it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n","    data sample for a given key. Subclasses could also optionally overwrite\n","    :meth:`__len__`, which is expected to return the size of the dataset by many\n","    :class:`~torch.utils.data.Sampler` implementations and the default options\n","    of :class:`~torch.utils.data.DataLoader`. Subclasses could also\n","    optionally implement :meth:`__getitems__`, for speedup batched samples\n","    loading. This method accepts list of indices of samples of batch and returns\n","    list of samples.\n","\n","    .. note::\n","      :class:`~torch.utils.data.DataLoader` by default constructs an index\n","      sampler that yields integral indices.  To make it work with a map-style\n","      dataset with non-integral indices/keys, a custom sampler must be provided.\n","    \"\"\"\n","\n","    def __getitem__(self, index) -> _T_co:\n","        raise NotImplementedError(\"Subclasses of Dataset should implement __getitem__.\")\n","\n","    # def __getitems__(self, indices: List) -> List[_T_co]:\n","    # Not implemented to prevent false-positives in fetcher check in\n","    # torch.utils.data._utils.fetch._MapDatasetFetcher\n","\n","    def __add__(self, other: \"Dataset[_T_co]\") -> \"ConcatDataset[_T_co]\":\n","        return ConcatDataset([self, other])\n","\n","    # No `def __len__(self)` default?\n","    # See NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\n","    # in pytorch/torch/utils/data/sampler.py\n","\n"]}]},{"cell_type":"markdown","source":["## Composition"],"metadata":{"id":"Xu9_zC6Kkamu"}},{"cell_type":"code","source":["class A:\n","  x = 1\n","  def y(self):\n","    print('y')\n","\n","# 합성: Composition, 여러 애들을 한꺼번에 명령할 수 있다.\n","# 다른 클래스의 인스턴스를 내 클래스의 인스턴스로 사용하는 방법(상속 대신 사용 가능)\n","# 명령 하나만으로 다른 클래스의 기능을 함께 작동시킬 수 있다.\n","class B:\n","  def __init__(self):\n","    self.a = A()"],"metadata":{"id":"BC1rr0kGT21D","executionInfo":{"status":"ok","timestamp":1741853652149,"user_tz":-540,"elapsed":43,"user":{"displayName":"Seonghyeok Jo","userId":"13534585656071255446"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TTR2L69Cj-H2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["파이썬의 추상화는 **상속**과 **metaclass**의 형태로 구현한다.\n","\n"],"metadata":{"id":"GdcOFxBvnr1W"}},{"cell_type":"code","source":[],"metadata":{"id":"R-670G2xoLCP"},"execution_count":null,"outputs":[]}]}